{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fetch SVGs to Google Drive (Colab)\n",
        "\n",
        "This notebook mounts Drive, lists SVG files on Wikimedia Commons, and downloads them directly into Drive with polite rate limiting.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1) Mount Drive\n",
        "from google.colab import drive  # type: ignore\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "DRIVE_DIR = '/content/drive/MyDrive/WikipediaSVG'  # change if needed\n",
        "import os\n",
        "os.makedirs(DRIVE_DIR, exist_ok=True)\n",
        "print('Drive dir:', DRIVE_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2) Lightweight Commons SVG lister (generator=allpages ns=6)\n",
        "import time, random, requests, os\n",
        "from typing import Dict, Iterator\n",
        "\n",
        "COMMONS_API = 'https://commons.wikimedia.org/w/api.php'\n",
        "HEADERS = {'User-Agent': 'SvgBot/0.1 (https://github.com/JacobAsmuth; jacobasmuth@gmail.com)'}\n",
        "\n",
        "def list_svg_pages(limit: int) -> Iterator[Dict[str, str]]:\n",
        "  prefixes = '0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
        "  per_page = 50\n",
        "  got = 0\n",
        "  for i, sc in enumerate(prefixes):\n",
        "    if got >= limit:\n",
        "      break\n",
        "    start = f'File:{sc}'\n",
        "    end = f'File:{prefixes[i+1]}' if i+1 < len(prefixes) else None\n",
        "    cont: Dict[str,str] = {}\n",
        "    while got < limit:\n",
        "      params = {\n",
        "        'action': 'query', 'format': 'json', 'generator': 'allpages',\n",
        "        'gapnamespace': '6', 'gaplimit': str(per_page), 'prop': 'imageinfo',\n",
        "        'iiprop': 'url|mime|size|timestamp', 'origin': '*', 'gapfrom': start,\n",
        "      }\n",
        "      if end is not None:\n",
        "        params['gapto'] = end\n",
        "      merged = {**params, **cont}\n",
        "      r = requests.get(COMMONS_API, params=merged, headers=HEADERS, timeout=60)\n",
        "      data = r.json()\n",
        "      pages = (data.get('query') or {}).get('pages') or {}\n",
        "      for page in pages.values():\n",
        "        title = page.get('title','')\n",
        "        if not title.lower().endswith(('.svg','.svgz')):\n",
        "          continue\n",
        "        infos = page.get('imageinfo') or []\n",
        "        if not infos: continue\n",
        "        url = infos[0].get('url')\n",
        "        mime = infos[0].get('mime','')\n",
        "        if not url or (mime != 'image/svg+xml' and not url.lower().endswith(('.svg','.svgz'))):\n",
        "          continue\n",
        "        got += 1\n",
        "        yield {'title': title, 'url': url}\n",
        "        if got >= limit: break\n",
        "      cont = data.get('continue') or {}\n",
        "      if not cont: break\n",
        "      time.sleep(0.3 + random.random()*0.2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3) Download to Drive (streamed)\n",
        "import hashlib\n",
        "import pathlib\n",
        "import requests\n",
        "\n",
        "SESSION = requests.Session()\n",
        "\n",
        "def download_to_drive(url: str, drive_dir: str, filename: str) -> bool:\n",
        "  p = pathlib.Path(drive_dir) / filename\n",
        "  if p.exists():\n",
        "    return False\n",
        "  with SESSION.get(url, stream=True, timeout=120, headers=HEADERS) as r:\n",
        "    r.raise_for_status()\n",
        "    with open(p, 'wb') as f:\n",
        "      for chunk in r.iter_content(chunk_size=1024*64):\n",
        "        if chunk:\n",
        "          f.write(chunk)\n",
        "  return True\n",
        "\n",
        "limit = 200  # adjust as needed\n",
        "saved = 0\n",
        "for item in list_svg_pages(limit):\n",
        "  title = item['title']\n",
        "  url = item['url']\n",
        "  # Save as basename from URL\n",
        "  fname = url.split('/')[-1]\n",
        "  ok = download_to_drive(url, DRIVE_DIR, fname)\n",
        "  if ok:\n",
        "    saved += 1\n",
        "    if saved % 10 == 0:\n",
        "      print(f'Saved {saved}/{limit}...')\n",
        "print('Done. Saved', saved)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
