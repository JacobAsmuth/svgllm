{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fetch SVGs to Google Drive (Colab)\n",
        "\n",
        "This notebook mounts Drive, lists SVG files on Wikimedia Commons, and downloads them directly into Drive with polite rate limiting.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1) Mount Drive\n",
        "from google.colab import drive  # type: ignore\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "DRIVE_DIR = '/content/drive/MyDrive/WikipediaSVG'  # change if needed\n",
        "import os\n",
        "os.makedirs(DRIVE_DIR, exist_ok=True)\n",
        "print('Drive dir:', DRIVE_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2) Lightweight Commons SVG lister (generator=allpages ns=6) with resume state in Drive\n",
        "import time, random, requests, os, json\n",
        "from typing import Dict, Iterator, Tuple\n",
        "\n",
        "COMMONS_API = 'https://commons.wikimedia.org/w/api.php'\n",
        "HEADERS = {'User-Agent': 'SvgBot/0.1 (https://github.com/JacobAsmuth; jacobasmuth@gmail.com)'}\n",
        "STATE_PATH = os.path.join(DRIVE_DIR, 'fetch_state.json')\n",
        "\n",
        "\n",
        "def load_state() -> Tuple[int, Dict[str, str]]:\n",
        "  if os.path.exists(STATE_PATH):\n",
        "    try:\n",
        "      with open(STATE_PATH, 'r') as f:\n",
        "        s = json.load(f)\n",
        "      return int(s.get('prefix_index', 0)), dict(s.get('cont', {}))\n",
        "    except Exception:\n",
        "      pass\n",
        "  return 0, {}\n",
        "\n",
        "\n",
        "def save_state(prefix_index: int, cont: Dict[str, str]) -> None:\n",
        "  try:\n",
        "    with open(STATE_PATH, 'w') as f:\n",
        "      json.dump({'prefix_index': int(prefix_index), 'cont': dict(cont)}, f)\n",
        "  except Exception:\n",
        "    pass\n",
        "\n",
        "\n",
        "def reset_state() -> None:\n",
        "  try:\n",
        "    os.remove(STATE_PATH)\n",
        "    print('State reset')\n",
        "  except FileNotFoundError:\n",
        "    print('State already clear')\n",
        "\n",
        "\n",
        "def list_svg_pages(limit: int, resume: bool = True) -> Iterator[Dict[str, str]]:\n",
        "  prefixes = '0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
        "  per_page = 50\n",
        "  got = 0\n",
        "  start_idx, persisted_cont = load_state() if resume else (0, {})\n",
        "\n",
        "  for i in range(start_idx, len(prefixes)):\n",
        "    sc = prefixes[i]\n",
        "    if got >= limit:\n",
        "      break\n",
        "    start = f'File:{sc}'\n",
        "    end = f'File:{prefixes[i+1]}' if i+1 < len(prefixes) else None\n",
        "    cont: Dict[str, str] = dict(persisted_cont) if persisted_cont else {}\n",
        "    persisted_cont = {}\n",
        "\n",
        "    while got < limit:\n",
        "      try:\n",
        "        params = {\n",
        "          'action': 'query', 'format': 'json', 'generator': 'allpages',\n",
        "          'gapnamespace': '6', 'gaplimit': str(per_page), 'prop': 'imageinfo',\n",
        "          'iiprop': 'url|mime|size|timestamp', 'origin': '*', 'gapfrom': start,\n",
        "        }\n",
        "        if end is not None:\n",
        "          params['gapto'] = end\n",
        "        merged = {**params, **cont}\n",
        "        r = requests.get(COMMONS_API, params=merged, headers=HEADERS, timeout=60)\n",
        "        data = r.json()\n",
        "        pages = (data.get('query') or {}).get('pages') or {}\n",
        "        cont = data.get('continue') or {}\n",
        "        save_state(i, cont)\n",
        "      except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        continue\n",
        "\n",
        "      for page in pages.values():\n",
        "        try:\n",
        "          title = page.get('title','')\n",
        "          if not title.lower().endswith(('.svg','.svgz')):\n",
        "            continue\n",
        "          infos = page.get('imageinfo') or []\n",
        "          if not infos: continue\n",
        "          url = infos[0].get('url')\n",
        "          mime = infos[0].get('mime','')\n",
        "          if not url or (mime != 'image/svg+xml' and not url.lower().endswith(('.svg','.svgz'))):\n",
        "            continue\n",
        "          got += 1\n",
        "          yield {'title': title, 'url': url}\n",
        "          if got >= limit: break\n",
        "        except Exception as e:\n",
        "          print(f\"Error: {e}\")\n",
        "          continue\n",
        "\n",
        "      if not cont:\n",
        "        save_state(i+1, {})\n",
        "        break\n",
        "      time.sleep(0.3 + random.random()*0.2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3) Download to Drive (streamed)\n",
        "import pathlib\n",
        "import requests\n",
        "\n",
        "SESSION = requests.Session()\n",
        "\n",
        "def download_to_drive(url: str, drive_dir: str, filename: str) -> bool:\n",
        "  p = pathlib.Path(drive_dir) / filename\n",
        "  if p.exists():\n",
        "    return False\n",
        "  with SESSION.get(url, stream=True, timeout=120, headers=HEADERS) as r:\n",
        "    r.raise_for_status()\n",
        "    with open(p, 'wb') as f:\n",
        "      for chunk in r.iter_content(chunk_size=1024*64):\n",
        "        if chunk:\n",
        "          f.write(chunk)\n",
        "  return True\n",
        "\n",
        "limit = 200  # adjust as needed\n",
        "resume = True  # set False to start over from the beginning\n",
        "saved = 0\n",
        "for item in list_svg_pages(limit, resume=resume):\n",
        "  url = item['url']\n",
        "  fname = url.split('/')[-1]\n",
        "  ok = download_to_drive(url, DRIVE_DIR, fname)\n",
        "  if ok:\n",
        "    saved += 1\n",
        "    if saved % 10 == 0:\n",
        "      print(f'Saved {saved}/{limit}...')\n",
        "print('Done. Saved', saved)\n",
        "print('Resume state saved at:', STATE_PATH)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
